{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook looks to replicate the tutorial material from databrick's own tutorials, only using open source libraries and tools.  \n",
    "  \n",
    "Below is a list of the tools used.  \n",
    "  \n",
    "For hive setup see: https://community.tableau.com/docs/DOC-7638\n",
    "Also, for creating metastore db tables in hive schematool was used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://192.168.1.123:8998  \n",
    "Livy Server: http://192.168.1.123:8998/ui  \n",
    "Spark Master: http://192.168.1.123:8080/  \n",
    "Spark Magic: https://github.com/jupyter-incubator/sparkmagic/blob/master/examples/Magics%20in%20IPython%20Kernel.ipynb  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load up sparkmagic to be able to communicate without spark cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sparkmagic.magics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's configure our connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc31631518e64f308c86c76ab85254e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MagicsControllerWidget(children=(Tab(children=(ManageSessionWidget(children=(HTML(value='<br/>'), HBox(childre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>3</td><td>None</td><td>spark</td><td>idle</td><td></td><td></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "%manage_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the data used in the tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: there was one deprecation warning; re-run with -deprecation for details\n",
      "sqlContext: org.apache.spark.sql.hive.HiveContext = org.apache.spark.sql.hive.HiveContext@90ac6a1\n",
      "marketsFile: org.apache.spark.rdd.RDD[String] = file:///home/churtado/notebooks/data/farmers_market.csv MapPartitionsRDD[54] at textFile at <console>:26\n",
      "taxes2013: org.apache.spark.sql.DataFrame = [STATEFIPS: string, STATE: string ... 129 more fields]\n",
      "res25: org.apache.spark.sql.DataFrame = []\n",
      "markets: org.apache.spark.sql.DataFrame = [FMID: string, MarketName: string ... 57 more fields]\n"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "val sqlContext = new org.apache.spark.sql.hive.HiveContext(sc)\n",
    "\n",
    "val marketsFile = sc.textFile(\"file:///home/churtado/notebooks/data/farmers_market.csv\")\n",
    "\n",
    "val taxes2013 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"file:///home/churtado/notebooks/data/zipcodes.csv\")\n",
    "taxes2013.createOrReplaceTempView(\"taxes2013\")\n",
    "\n",
    "val markets = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"file:///home/churtado/notebooks/data/farmers_market.csv\")\n",
    "markets.createOrReplaceTempView(\"markets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use sql magic to view the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>databaseName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>default</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  databaseName\n",
       "0      default"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%spark -c sql\n",
    "SHOW SCHEMAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>database</th>\n",
       "      <th>tableName</th>\n",
       "      <th>isTemporary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaT</td>\n",
       "      <td>markets</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaT</td>\n",
       "      <td>taxes2013</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  database  tableName  isTemporary\n",
       "0      NaT    markets         True\n",
       "1      NaT  taxes2013         True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%spark -c sql\n",
    "SHOW TABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark -c sql -q -o df_markets \n",
    "SELECT * FROM markets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use pandas dataframes for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FMID</th>\n",
       "      <th>MarketName</th>\n",
       "      <th>Website</th>\n",
       "      <th>Facebook</th>\n",
       "      <th>city</th>\n",
       "      <th>County</th>\n",
       "      <th>State</th>\n",
       "      <th>zip</th>\n",
       "      <th>Season1Date</th>\n",
       "      <th>Season1Time</th>\n",
       "      <th>...</th>\n",
       "      <th>Tofu</th>\n",
       "      <th>WildHarvested</th>\n",
       "      <th>updateTime</th>\n",
       "      <th>street</th>\n",
       "      <th>OtherMedia</th>\n",
       "      <th>Location</th>\n",
       "      <th>Twitter</th>\n",
       "      <th>Youtube</th>\n",
       "      <th>Season3Date</th>\n",
       "      <th>Season3Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1018261</td>\n",
       "      <td>Caledonia Farmers Market Association - Danville</td>\n",
       "      <td>https://sites.google.com/site/caledoniafarmers...</td>\n",
       "      <td>https://www.facebook.com/Danville.VT.Farmers.M...</td>\n",
       "      <td>Danville</td>\n",
       "      <td>Caledonia</td>\n",
       "      <td>Vermont</td>\n",
       "      <td>05828</td>\n",
       "      <td>06/14/2017 to 08/30/2017</td>\n",
       "      <td>Wed: 9:00 AM-1:00 PM;</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>2017-06-20 22:43:57</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1018318</td>\n",
       "      <td>Stearns Homestead Farmers' Market</td>\n",
       "      <td>http://www.StearnsHomestead.com</td>\n",
       "      <td>StearnsHomesteadFarmersMarket</td>\n",
       "      <td>Parma</td>\n",
       "      <td>Cuyahoga</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>06/24/2017 to 09/30/2017</td>\n",
       "      <td>Sat: 9:00 AM-1:00 PM;</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>2017-06-21 17:15:01</td>\n",
       "      <td>6975 Ridge Road</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1009364</td>\n",
       "      <td>106 S. Main Street Farmers Market</td>\n",
       "      <td>http://thetownofsixmile.wordpress.com/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Six Mile</td>\n",
       "      <td>NaN</td>\n",
       "      <td>South Carolina</td>\n",
       "      <td>29682</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>2013-01-01 00:00:00</td>\n",
       "      <td>106 S. Main Street</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      FMID                                        MarketName  \\\n",
       "0  1018261   Caledonia Farmers Market Association - Danville   \n",
       "1  1018318                 Stearns Homestead Farmers' Market   \n",
       "2  1009364                 106 S. Main Street Farmers Market   \n",
       "\n",
       "                                             Website  \\\n",
       "0  https://sites.google.com/site/caledoniafarmers...   \n",
       "1                    http://www.StearnsHomestead.com   \n",
       "2             http://thetownofsixmile.wordpress.com/   \n",
       "\n",
       "                                            Facebook      city     County  \\\n",
       "0  https://www.facebook.com/Danville.VT.Farmers.M...  Danville  Caledonia   \n",
       "1                      StearnsHomesteadFarmersMarket    Parma    Cuyahoga   \n",
       "2                                                NaN  Six Mile        NaN   \n",
       "\n",
       "            State    zip               Season1Date            Season1Time  \\\n",
       "0         Vermont  05828  06/14/2017 to 08/30/2017  Wed: 9:00 AM-1:00 PM;   \n",
       "1            Ohio    NaN  06/24/2017 to 09/30/2017  Sat: 9:00 AM-1:00 PM;   \n",
       "2  South Carolina  29682                       NaN                    NaN   \n",
       "\n",
       "      ...     Tofu WildHarvested          updateTime              street  \\\n",
       "0     ...        N             N 2017-06-20 22:43:57                 NaN   \n",
       "1     ...        N             N 2017-06-21 17:15:01     6975 Ridge Road   \n",
       "2     ...        N             N 2013-01-01 00:00:00  106 S. Main Street   \n",
       "\n",
       "  OtherMedia Location Twitter Youtube Season3Date Season3Time  \n",
       "0        NaN      NaN     NaN     NaN         NaN         NaN  \n",
       "1        NaN      NaN     NaN     NaN         NaN         NaN  \n",
       "2        NaN      NaN     NaN     NaN         NaN         NaN  \n",
       "\n",
       "[3 rows x 57 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_markets.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the data as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27a6f98b1e674771b678d1a8b8899225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(HTML(value='Type:'), Button(description='Table', layout=Layout(width='70px'), st…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18c415383a3846eda90dde4715ba7327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac18c846f289452ba3fae64b7e6e2e8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "AutoVizWidget()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from autovizwidget.widget.utils import display_dataframe\n",
    "display_dataframe(df_markets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to clean up the data with SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%spark -c sql\n",
    "DROP TABLE IF EXISTS cleaned_taxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%spark -c sql -q -o df_cleaned_taxes\n",
    "\n",
    "CREATE TABLE cleaned_taxes AS\n",
    "SELECT state, int(zipcode / 10) as zipcode, \n",
    "  int(mars1) as single_returns, \n",
    "  int(mars2) as joint_returns, \n",
    "  int(numdep) as numdep, \n",
    "  double(A02650) as total_income_amount,\n",
    "  double(A00300) as taxable_interest_amount,\n",
    "  double(a01000) as net_capital_gains,\n",
    "  double(a00900) as biz_net_income\n",
    "FROM taxes2013"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to look at avg income per state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%spark -c sql\n",
    "DROP TABLE IF EXISTS income_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark -c sql -q \n",
    "\n",
    "CREATE TABLE income_state AS\n",
    "SELECT state, \n",
    "sum(total_income_amount) / sum(1) as total_income\n",
    "FROM cleaned_taxes\n",
    "group by state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark -c sql -q  -o df_income_state\n",
    "\n",
    "select * from income_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to plot income by state using plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "plotly.tools.set_credentials_file(username='churtado', api_key='iaMRV6ydU9Ove5Yfy0R7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~churtado/6.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states = df_income_state[\"state\"]\n",
    "values = df_income_state[\"total_income\"]\n",
    "\n",
    "import plotly.plotly as py\n",
    "import pandas as pd\n",
    "\n",
    "data = [ dict(\n",
    "        type='choropleth',\n",
    "        autocolorscale = True,\n",
    "        locations = df_income_state['state'],\n",
    "        z = df_income_state['total_income'].astype(float),\n",
    "        locationmode = 'USA-states',\n",
    "        marker = dict(\n",
    "            line = dict (\n",
    "                color = 'rgb(255,255,255)',\n",
    "                width = 2\n",
    "            ) ),\n",
    "        colorbar = dict(\n",
    "            title = \"Total Avg Income\")\n",
    "        ) ]\n",
    "\n",
    "layout = dict(\n",
    "        title = 'Map of avg income',\n",
    "        geo = dict(\n",
    "            scope='usa',\n",
    "            projection=dict( type='albers usa' ),\n",
    "            showlakes = True,\n",
    "            lakecolor = 'rgb(255, 255, 255)'),\n",
    "             )\n",
    "    \n",
    "fig = dict( data=data, layout=layout )\n",
    "py.iplot( fig, filename='d3-cloropleth-map' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>single_returns</th>\n",
       "      <th>joint_returns</th>\n",
       "      <th>numdep</th>\n",
       "      <th>total_income_amount</th>\n",
       "      <th>taxable_interest_amount</th>\n",
       "      <th>net_capital_gains</th>\n",
       "      <th>biz_net_income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AL</td>\n",
       "      <td>0</td>\n",
       "      <td>481570</td>\n",
       "      <td>109790</td>\n",
       "      <td>525260</td>\n",
       "      <td>11036309.0</td>\n",
       "      <td>59467.0</td>\n",
       "      <td>34276.0</td>\n",
       "      <td>805250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AL</td>\n",
       "      <td>0</td>\n",
       "      <td>206630</td>\n",
       "      <td>146250</td>\n",
       "      <td>372110</td>\n",
       "      <td>17960153.0</td>\n",
       "      <td>66324.0</td>\n",
       "      <td>66922.0</td>\n",
       "      <td>255898.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AL</td>\n",
       "      <td>0</td>\n",
       "      <td>80720</td>\n",
       "      <td>139280</td>\n",
       "      <td>183750</td>\n",
       "      <td>16216787.0</td>\n",
       "      <td>65157.0</td>\n",
       "      <td>100439.0</td>\n",
       "      <td>256170.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AL</td>\n",
       "      <td>0</td>\n",
       "      <td>28510</td>\n",
       "      <td>124650</td>\n",
       "      <td>132360</td>\n",
       "      <td>14588959.0</td>\n",
       "      <td>60620.0</td>\n",
       "      <td>126599.0</td>\n",
       "      <td>239715.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AL</td>\n",
       "      <td>0</td>\n",
       "      <td>19520</td>\n",
       "      <td>184320</td>\n",
       "      <td>192840</td>\n",
       "      <td>28985528.0</td>\n",
       "      <td>144182.0</td>\n",
       "      <td>513050.0</td>\n",
       "      <td>643920.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  state  zipcode  single_returns  joint_returns  numdep  total_income_amount  \\\n",
       "0    AL        0          481570         109790  525260           11036309.0   \n",
       "1    AL        0          206630         146250  372110           17960153.0   \n",
       "2    AL        0           80720         139280  183750           16216787.0   \n",
       "3    AL        0           28510         124650  132360           14588959.0   \n",
       "4    AL        0           19520         184320  192840           28985528.0   \n",
       "\n",
       "   taxable_interest_amount  net_capital_gains  biz_net_income  \n",
       "0                  59467.0            34276.0        805250.0  \n",
       "1                  66324.0            66922.0        255898.0  \n",
       "2                  65157.0           100439.0        256170.0  \n",
       "3                  60620.0           126599.0        239715.0  \n",
       "4                 144182.0           513050.0        643920.0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%spark -c sql \n",
    "\n",
    "SELECT * FROM cleaned_taxes limit 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark -c sql -q -o df_capital_gains\n",
    "\n",
    "SELECT zipcode AS zipcode, SUM(net_capital_gains) AS cap_gains\n",
    "FROM cleaned_taxes \n",
    "  WHERE NOT (zipcode = 0000 OR zipcode = 9999)\n",
    "GROUP BY zipcode\n",
    "ORDER BY cap_gains ASC\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the set of zip codes with the lowest total capital gains and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~churtado/10.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "trace1 = go.Bar(\n",
    "    x=df_capital_gains[\"zipcode\"],\n",
    "    y=df_capital_gains[\"cap_gains\"]\n",
    ")\n",
    "data = [trace1]\n",
    "layout = go.Layout(\n",
    "    xaxis=dict(\n",
    "        showgrid=False,\n",
    "        type='category',\n",
    "        zeroline=False,\n",
    "        showline=True,\n",
    "        mirror='ticks',\n",
    "        gridcolor='#bdbdbd',\n",
    "        gridwidth=1,\n",
    "        zerolinecolor='#969696',\n",
    "        zerolinewidth=4,\n",
    "        linecolor='#636363',\n",
    "        #linewidth=6\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        showgrid=False,\n",
    "        zeroline=False,\n",
    "        showline=True,\n",
    "        mirror='ticks',\n",
    "        gridcolor='#bdbdbd',\n",
    "        gridwidth=1,\n",
    "        zerolinecolor='#969696',\n",
    "        zerolinewidth=1,\n",
    "        linecolor='#636363',\n",
    "        #linewidth=6\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='axes-lines')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a combination of capital gains and business net income to see what we find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark -c sql -q -o df_capital_and_business_income\n",
    "\n",
    "SELECT zipcode, \n",
    "  SUM(biz_net_income) as business_net_income, \n",
    "  SUM(net_capital_gains) as capital_gains, \n",
    "  SUM(net_capital_gains) + SUM(biz_net_income) as capital_and_business_income\n",
    "FROM cleaned_taxes \n",
    "  WHERE NOT (zipcode = 0000 OR zipcode = 9999)\n",
    "GROUP BY zipcode\n",
    "ORDER BY capital_and_business_income DESC\n",
    "LIMIT 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~churtado/12.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "trace1 = go.Bar(\n",
    "    x=df_capital_and_business_income[\"zipcode\"],\n",
    "    y=df_capital_and_business_income[\"business_net_income\"],\n",
    "    name='Business Net Income'\n",
    ")\n",
    "\n",
    "trace2 = go.Bar(\n",
    "    x=df_capital_and_business_income[\"zipcode\"],\n",
    "    y=df_capital_and_business_income[\"capital_gains\"],\n",
    "    name='Capital Gains'\n",
    ")\n",
    "\n",
    "trace3 = go.Bar(\n",
    "    x=df_capital_and_business_income[\"zipcode\"],\n",
    "    y=df_capital_and_business_income[\"capital_and_business_income\"],\n",
    "    name='Capital and Business Income'\n",
    ")\n",
    "\n",
    "data = [trace1, trace2, trace3]\n",
    "layout = go.Layout(\n",
    "    barmode='group',\n",
    "    xaxis=dict(\n",
    "        showgrid=False,\n",
    "        type='category',\n",
    "        zeroline=False,\n",
    "        showline=True,\n",
    "        mirror='ticks',\n",
    "        gridcolor='#bdbdbd',\n",
    "        gridwidth=1,\n",
    "        zerolinecolor='#969696',\n",
    "        zerolinewidth=4,\n",
    "        linecolor='#636363',\n",
    "        #linewidth=6\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        showgrid=False,\n",
    "        zeroline=False,\n",
    "        showline=True,\n",
    "        mirror='ticks',\n",
    "        gridcolor='#bdbdbd',\n",
    "        gridwidth=1,\n",
    "        zerolinecolor='#969696',\n",
    "        zerolinewidth=1,\n",
    "        linecolor='#636363',\n",
    "        #linewidth=6\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='kpis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to see how a query will run we can look at the plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark -c sql -q -o df_explain_plan\n",
    "EXPLAIN \n",
    "  SELECT zipcode, \n",
    "    SUM(biz_net_income) as net_income, \n",
    "    SUM(net_capital_gains) as cap_gains, \n",
    "    SUM(net_capital_gains) + SUM(biz_net_income) as combo\n",
    "  FROM cleaned_taxes \n",
    "  WHERE NOT (zipcode = 0000 OR zipcode = 9999)\n",
    "  GROUP BY zipcode\n",
    "  ORDER BY combo desc\n",
    "  limit 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    == Physical Plan ==\n",
      "TakeOrderedAndProject(limit=50, orderBy=[combo#714 DESC NULLS LAST], output=[zipcode#718,net_income#712,cap_gains#713,combo#714])\n",
      "+- *(2) HashAggregate(keys=[zipcode#718], functions=[sum(biz_net_income#725), sum(net_capital_gains#724)])\n",
      "   +- Exchange hashpartitioning(zipcode#718, 200)\n",
      "      +- *(1) HashAggregate(keys=[zipcode#718], functions=[partial_sum(biz_net_income#725), partial_sum(net_capital_gains#724)])\n",
      "         +- *(1) Filter ((isnotnull(zipcode#718) && NOT (zipcode#718 = 0)) && NOT (zipcode#718 = 9999))\n",
      "            +- HiveTableScan [zipcode#718, net_capital_gains#724, biz_net_income#725], HiveTableRelation `default`.`cleaned_taxes`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [state#717, zipcode#718, single_returns#719, joint_returns#720, numdep#721, total_income_amount#722, taxable_interest_amount#723, net_capital_gains#724, biz_net_income#725]"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 10000)\n",
    "plan = df_explain_plan[\"plan\"].to_string().replace('\\\\n', '\\n')\n",
    "sys.stdout.write(plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can cache the table in memory. If we do it with SQL it's done eagerly, and with the API lazily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res19: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [state: string, zipcode: int ... 7 more fields]\n"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "spark.table(\"cleaned_taxes\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%spark -c sql \n",
    "\n",
    "CACHE TABLE cleaned_taxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we query it now, the query will be faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark -c sql  -q \n",
    "\n",
    "SELECT zipcode, \n",
    "  SUM(biz_net_income) as net_income, \n",
    "  SUM(net_capital_gains) as cap_gains, \n",
    "  SUM(net_capital_gains) + SUM(biz_net_income) as combo\n",
    "FROM cleaned_taxes \n",
    "  WHERE NOT (zipcode = 0000 OR zipcode = 9999)\n",
    "GROUP BY zipcode\n",
    "ORDER BY combo desc\n",
    "limit 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will have a different explain plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark -c sql -q -o df_explain_plan2\n",
    "EXPLAIN \n",
    "  SELECT zipcode, \n",
    "    SUM(biz_net_income) as net_income, \n",
    "    SUM(net_capital_gains) as cap_gains, \n",
    "    SUM(net_capital_gains) + SUM(biz_net_income) as combo\n",
    "  FROM cleaned_taxes \n",
    "  WHERE NOT (zipcode = 0000 OR zipcode = 9999)\n",
    "  GROUP BY zipcode\n",
    "  ORDER BY combo desc\n",
    "  limit 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    == Physical Plan ==\n",
      "TakeOrderedAndProject(limit=50, orderBy=[combo#983 DESC NULLS LAST], output=[zipcode#988,net_income#981,cap_gains#982,combo#983])\n",
      "+- *(2) HashAggregate(keys=[zipcode#988], functions=[sum(biz_net_income#995), sum(net_capital_gains#994)])\n",
      "   +- Exchange hashpartitioning(zipcode#988, 200)\n",
      "      +- *(1) HashAggregate(keys=[zipcode#988], functions=[partial_sum(biz_net_income#995), partial_sum(net_capital_gains#994)])\n",
      "         +- *(1) Filter ((isnotnull(zipcode#988) && NOT (zipcode#988 = 0)) && NOT (zipcode#988 = 9999))\n",
      "            +- InMemoryTableScan [zipcode#988, net_capital_gains#994, biz_net_income#995], [isnotnull(zipcode#988), NOT (zipcode#988 = 0), NOT (zipcode#988 = 9999)]\n",
      "                  +- InMemoryRelation [state#987, zipcode#988, single_returns#989, joint_returns#990, numdep#991, total_income_amount#992, taxable_interest_amount#993, net_capital_gains#994, biz_net_income#995], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                        +- HiveTableScan [state#741, zipcode#742, single_returns#743, joint_returns#744, numdep#745, total_income_amount#746, taxable_interest_amount#747, net_capital_gains#748, biz_net_income#749], HiveTableRelation `default`.`cleaned_taxes`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [state#741, zipcode#742, single_returns#743, joint_returns#744, numdep#745, total_income_amount#746, taxable_interest_amount#747, net_capital_gains#748, biz_net_income#749]"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 10000)\n",
    "plan = df_explain_plan2[\"plan\"].to_string().replace('\\\\n', '\\n')\n",
    "sys.stdout.write(plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the farmer's market data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark -c sql -q -o df_markets\n",
    "SELECT state, count(*) as count FROM markets GROUP BY state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~churtado/14.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "trace1 = go.Bar(\n",
    "    x=df_markets[\"state\"],\n",
    "    y=df_markets[\"count\"]\n",
    ")\n",
    "data = [trace1]\n",
    "layout = go.Layout(\n",
    "    xaxis=dict(\n",
    "        showgrid=False,\n",
    "        type='category',\n",
    "        zeroline=False,\n",
    "        showline=True,\n",
    "        mirror='ticks',\n",
    "        gridcolor='#bdbdbd',\n",
    "        gridwidth=1,\n",
    "        zerolinecolor='#969696',\n",
    "        zerolinewidth=4,\n",
    "        linecolor='#636363',\n",
    "        #linewidth=6\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        showgrid=False,\n",
    "        zeroline=False,\n",
    "        showline=True,\n",
    "        mirror='ticks',\n",
    "        gridcolor='#bdbdbd',\n",
    "        gridwidth=1,\n",
    "        zerolinecolor='#969696',\n",
    "        zerolinewidth=1,\n",
    "        linecolor='#636363',\n",
    "        #linewidth=6\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='markets1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could explore more, but fuck it. Let's do some machine learning. Basically our categorical values will have to be converted to numerics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleanedTaxes: org.apache.spark.sql.DataFrame = [state: string, zipcode: int ... 7 more fields]\n",
      "summedTaxes: org.apache.spark.sql.DataFrame = [zipcode: int, sum(zipcode): bigint ... 7 more fields]\n",
      "cleanedMarkets: org.apache.spark.sql.DataFrame = [count: double, zip: int]\n",
      "joined: org.apache.spark.sql.DataFrame = [count: double, zip: int ... 9 more fields]\n",
      "res29: org.apache.spark.sql.DataFrame = [count: double, zip: int ... 9 more fields]\n"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "val cleanedTaxes = spark.sql(\"SELECT * FROM cleaned_taxes\")\n",
    "\n",
    "val summedTaxes = cleanedTaxes.groupBy(\"zipcode\").sum()\n",
    "\n",
    "// selectExpr is short for Select Expression, basically running a select on SQL\n",
    "val cleanedMarkets = markets.selectExpr(\"*\", \"int(zip / 10) as zipcode\").groupBy(\"zipcode\").count().selectExpr(\"double(count) as count\", \"zipcode as zip\")\n",
    "\n",
    "val joined = cleanedMarkets.join(summedTaxes, cleanedMarkets(\"zip\") === summedTaxes(\"zipcode\"), \"outer\")\n",
    "\n",
    "joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "joined.createOrReplaceTempView(\"joined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLLIB doesn't accept null values, so we'll convert our nulls to zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepped: org.apache.spark.sql.DataFrame = [count: double, zip: int ... 9 more fields]\n"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "val prepped = joined.na.fill(0)\n",
    "prepped.createOrReplaceTempView(\"prepped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>zip</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>sum(zipcode)</th>\n",
       "      <th>sum(single_returns)</th>\n",
       "      <th>sum(joint_returns)</th>\n",
       "      <th>sum(numdep)</th>\n",
       "      <th>sum(total_income_amount)</th>\n",
       "      <th>sum(taxable_interest_amount)</th>\n",
       "      <th>sum(net_capital_gains)</th>\n",
       "      <th>sum(biz_net_income)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>463</td>\n",
       "      <td>8334</td>\n",
       "      <td>960</td>\n",
       "      <td>830</td>\n",
       "      <td>1020</td>\n",
       "      <td>89634.0</td>\n",
       "      <td>334.0</td>\n",
       "      <td>1118.0</td>\n",
       "      <td>5062.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>496</td>\n",
       "      <td>496</td>\n",
       "      <td>17856</td>\n",
       "      <td>3320</td>\n",
       "      <td>3170</td>\n",
       "      <td>4070</td>\n",
       "      <td>378755.0</td>\n",
       "      <td>1198.0</td>\n",
       "      <td>5118.0</td>\n",
       "      <td>12375.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>833</td>\n",
       "      <td>9996</td>\n",
       "      <td>14000</td>\n",
       "      <td>9350</td>\n",
       "      <td>19460</td>\n",
       "      <td>1547173.0</td>\n",
       "      <td>6157.0</td>\n",
       "      <td>16364.0</td>\n",
       "      <td>38071.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1342</td>\n",
       "      <td>1342</td>\n",
       "      <td>40260</td>\n",
       "      <td>4980</td>\n",
       "      <td>3520</td>\n",
       "      <td>5110</td>\n",
       "      <td>509883.0</td>\n",
       "      <td>2430.0</td>\n",
       "      <td>11426.0</td>\n",
       "      <td>12840.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1580</td>\n",
       "      <td>1580</td>\n",
       "      <td>9480</td>\n",
       "      <td>4550</td>\n",
       "      <td>3910</td>\n",
       "      <td>4640</td>\n",
       "      <td>534223.0</td>\n",
       "      <td>3949.0</td>\n",
       "      <td>10545.0</td>\n",
       "      <td>23471.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count   zip  zipcode  sum(zipcode)  sum(single_returns)  \\\n",
       "0    0.0     0      463          8334                  960   \n",
       "1    2.0   496      496         17856                 3320   \n",
       "2    0.0     0      833          9996                14000   \n",
       "3    1.0  1342     1342         40260                 4980   \n",
       "4    1.0  1580     1580          9480                 4550   \n",
       "\n",
       "   sum(joint_returns)  sum(numdep)  sum(total_income_amount)  \\\n",
       "0                 830         1020                   89634.0   \n",
       "1                3170         4070                  378755.0   \n",
       "2                9350        19460                 1547173.0   \n",
       "3                3520         5110                  509883.0   \n",
       "4                3910         4640                  534223.0   \n",
       "\n",
       "   sum(taxable_interest_amount)  sum(net_capital_gains)  sum(biz_net_income)  \n",
       "0                         334.0                  1118.0               5062.0  \n",
       "1                        1198.0                  5118.0              12375.0  \n",
       "2                        6157.0                 16364.0              38071.0  \n",
       "3                        2430.0                 11426.0              12840.0  \n",
       "4                        3949.0                 10545.0              23471.0  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%spark -c sql\n",
    "select * from prepped limit 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is ready, lets conver into a vector type, so we can embed a prediction into it easily. We'll remove what isn't a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nonFeatureCols: Array[String] = Array(zip, zipcode, count)\n",
      "featureCols: Array[String] = Array(sum(zipcode), sum(single_returns), sum(joint_returns), sum(numdep), sum(total_income_amount), sum(taxable_interest_amount), sum(net_capital_gains), sum(biz_net_income))\n"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "val nonFeatureCols = Array(\"zip\", \"zipcode\", \"count\")\n",
    "val featureCols = prepped.columns.diff(nonFeatureCols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use VectorAssembler so all columns go in a single vector: set up the input and output columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'm going to use the VectorAssembler in Apache Spark to Assemble all of these columns into one single vector. To do this I'll have to set the input columns and output column. Then I'll use that assembler to transform the prepped data to my final dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.ml.feature.VectorAssembler\n",
      "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_f5255ea8631f\n",
      "finalPrep: org.apache.spark.sql.DataFrame = [count: double, zip: int ... 10 more fields]\n"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "val assembler = new VectorAssembler().setInputCols(featureCols).setOutputCol(\"features\")\n",
    "\n",
    "val finalPrep = assembler.transform(prepped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [count: double, zip: int ... 10 more fields]\n",
      "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [count: double, zip: int ... 10 more fields]\n",
      "res37: training.type = [count: double, zip: int ... 10 more fields]\n",
      "res38: test.type = [count: double, zip: int ... 10 more fields]\n",
      "3979\n",
      "1830\n"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "val Array(training, test) = finalPrep.randomSplit(Array(0.7, 0.3))\n",
    "\n",
    "// Going to cache the data to make sure things stay snappy!\n",
    "training.cache()\n",
    "test.cache()\n",
    "\n",
    "println(training.count())\n",
    "println(test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do some machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.ml.regression.LinearRegression\n",
      "lrModel: org.apache.spark.ml.regression.LinearRegression = linReg_387ede8ea192\n",
      "Printing out the model Parameters:\n",
      "--------------------\n",
      "aggregationDepth: suggested depth for treeAggregate (>= 2) (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty (default: 0.0, current: 0.5)\n",
      "epsilon: The shape parameter to control the amount of robustness. Must be > 1.0. (default: 1.35)\n",
      "featuresCol: features column name (default: features, current: features)\n",
      "fitIntercept: whether to fit an intercept term (default: true)\n",
      "labelCol: label column name (default: label, current: count)\n",
      "loss: The loss function to be optimized. Supported options: squaredError, huber. (Default squaredError) (default: squaredError)\n",
      "maxIter: maximum number of iterations (>= 0) (default: 100)\n",
      "predictionCol: prediction column name (default: prediction)\n",
      "regParam: regularization parameter (>= 0) (default: 0.0)\n",
      "solver: The solver algorithm for optimization. Supported options: auto, normal, l-bfgs. (Default auto) (default: auto)\n",
      "standardization: whether to standardize the training features before fitting the model (default: true)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0) (default: 1.0E-6)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0 (undefined)\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "\n",
    "val lrModel = new LinearRegression().setLabelCol(\"count\").setFeaturesCol(\"features\").setElasticNetParam(0.5)\n",
    "\n",
    "println(\"Printing out the model Parameters:\")\n",
    "println(\"-\"*20)\n",
    "println(lrModel.explainParams)\n",
    "println(\"-\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see what our model is going to do, now let's train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.mllib.evaluation.RegressionMetrics\n",
      "lrFitted: org.apache.spark.ml.regression.LinearRegressionModel = linReg_f3f974351ee0\n"
     ]
    }
   ],
   "source": [
    "%%spark \n",
    "\n",
    "import org.apache.spark.mllib.evaluation.RegressionMetrics\n",
    "val lrFitted = lrModel.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you'll see that since we're working with exact numbers (you can't have 1/2 a farmer's market for example), I'm going to check equality by first rounding the value to the nearest digital value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "holdout: org.apache.spark.sql.DataFrame = [raw_prediction: double, prediction: double ... 2 more fields]\n"
     ]
    }
   ],
   "source": [
    "%%spark \n",
    "val holdout = lrFitted.transform(test).selectExpr(\"prediction as raw_prediction\", \"double(round(prediction)) as prediction\", \"count\", \"\"\"CASE double(round(prediction)) = count WHEN true then 1 ELSE 0 END as equal\"\"\")\n",
    "\n",
    "holdout.createOrReplaceTempView(\"holdout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_prediction</th>\n",
       "      <th>prediction</th>\n",
       "      <th>count</th>\n",
       "      <th>equal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.083217</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.117802</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.049796</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.917156</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.226193</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   raw_prediction  prediction  count  equal\n",
       "0        1.083217         1.0    0.0      0\n",
       "1        1.117802         1.0    0.0      0\n",
       "2        1.049796         1.0    1.0      1\n",
       "3        1.917156         2.0    1.0      0\n",
       "4        1.226193         1.0    1.0      1"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%spark -c sql\n",
    "select * from holdout limit 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now let's see what proportion was exactly correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proportion: org.apache.spark.sql.DataFrame = [(sum(equal) / sum(1)): double]\n"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "val proportion = holdout.selectExpr(\"sum(equal)/sum(1)\")\n",
    "\n",
    "proportion.createOrReplaceTempView(\"proportion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(sum(equal) / sum(1))</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.277066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   (sum(equal) / sum(1))\n",
       "0               0.277066"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%spark -c sql\n",
    "select * from proportion limit 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also calculate some regression metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: org.apache.spark.mllib.evaluation.RegressionMetrics = org.apache.spark.mllib.evaluation.RegressionMetrics@1c490988\n",
      "MSE: 533.3771827706636\n",
      "MAE: 1.6181606519208382\n",
      "RMSE Squared: 23.09496011623886\n",
      "R Squared: -4.162801670282157E-4\n",
      "Explained Variance: 1.0743944484273211\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "// have to do a type conversion for RegressionMetrics\n",
    "val rm = new RegressionMetrics(\n",
    "  holdout.select(\"prediction\", \"count\").rdd.map(x =>\n",
    "  (x(0).asInstanceOf[Double], x(1).asInstanceOf[Double])))\n",
    "\n",
    "println(\"MSE: \" + rm.meanSquaredError)\n",
    "println(\"MAE: \" + rm.meanAbsoluteError)\n",
    "println(\"RMSE Squared: \" + rm.rootMeanSquaredError)\n",
    "println(\"R Squared: \" + rm.r2)\n",
    "println(\"Explained Variance: \" + rm.explainedVariance + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found these results to be sub-optimal, so let's try exploring another way to train the model. Rather than training on a single model with hard-coded parameters, let's train using a pipeline.  \n",
    "  \n",
    "A pipeline is going to give us some nice benefits in that it will allow us to couple of transformations we need in order to transform our raw data into the prepared data for the model but also it provides a simple, straightforward way to try out a lot of different combinations of parameters. This is a process called hyperparameter tuning or grid search. To review, grid search is where you set up the exact parameters that you would like to test and MLLib will automatically create all the necessary combinations of these to test.  \n",
    "  \n",
    "For example, below we'll see numTrees to 20 and 60 and maxDepth to 5 and 10. The parameter grid builder will automatically construct all the combinations of these two variable (along with the other ones that we might specify too). Additionally we're also going to use cross validation) to tune our hyperparameters, this will allow us to attempt to try to control overfitting of our model.  \n",
    "  \n",
    "Lastly we'll need to set up a Regression Evaluator that will evaluate the models that we choose based on some metric (the default is RMSE). The key take away is that the pipeline will automatically optimize for our given metric choice by exploring the parameter grid that we set up rather than us having to do it manually like we would have had to do above.  \n",
    "  \n",
    "Now we can go about training our random forest!  \n",
    "  \n",
    "note: this might take a little while because of the number of combinations that we're trying and limitations in workers available.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.ml.regression.RandomForestRegressor\n",
      "import org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}\n",
      "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
      "import org.apache.spark.ml.{Pipeline, PipelineStage}\n",
      "rfModel: org.apache.spark.ml.regression.RandomForestRegressor = rfr_5cf5b7f034cd\n",
      "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
      "Array({\n",
      "\trfr_5cf5b7f034cd-maxDepth: 5,\n",
      "\trfr_5cf5b7f034cd-numTrees: 20\n",
      "}, {\n",
      "\trfr_5cf5b7f034cd-maxDepth: 5,\n",
      "\trfr_5cf5b7f034cd-numTrees: 60\n",
      "}, {\n",
      "\trfr_5cf5b7f034cd-maxDepth: 10,\n",
      "\trfr_5cf5b7f034cd-numTrees: 20\n",
      "}, {\n",
      "\trfr_5cf5b7f034cd-maxDepth: 10,\n",
      "\trfr_5cf5b7f034cd-numTrees: 60\n",
      "})\n",
      "steps: Array[org.apache.spark.ml.PipelineStage] = Array(rfr_5cf5b7f034cd)\n",
      "pipeline: org.apache.spark.ml.Pipeline = pipeline_53234c1a4fb5\n",
      "cv: org.apache.spark.ml.tuning.CrossValidator = cv_061280cacd8b\n",
      "pipelineFitted: org.apache.spark.ml.tuning.CrossValidatorModel = cv_061280cacd8b\n"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "import org.apache.spark.ml.regression.RandomForestRegressor\n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}\n",
    "\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "\n",
    "import org.apache.spark.ml.{Pipeline, PipelineStage}\n",
    "\n",
    "val rfModel = new RandomForestRegressor().setLabelCol(\"count\").setFeaturesCol(\"features\")\n",
    "\n",
    "val paramGrid = new ParamGridBuilder().addGrid(rfModel.maxDepth, Array(5, 10)).addGrid(rfModel.numTrees, Array(20, 60)).build()\n",
    "// Note, that this parameter grid will take a long time\n",
    "// to run in the community edition due to limited number\n",
    "// of workers available! Be patient for it to run!\n",
    "// If you want it to run faster, remove some of\n",
    "// the above parameters and it'll speed right up!\n",
    "\n",
    "val steps:Array[PipelineStage] = Array(rfModel)\n",
    "\n",
    "val pipeline = new Pipeline().setStages(steps)\n",
    "\n",
    "// you can feel free to change the number of folds used in cross validation as well\n",
    "// the estimator can also just be an individual model rather than a pipeline\n",
    "val cv = new CrossValidator().setEstimator(pipeline).setEstimatorParamMaps(paramGrid).setEvaluator(new RegressionEvaluator().setLabelCol(\"count\"))\n",
    "\n",
    "val pipelineFitted = cv.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've trained our model! Let's take a look at which version performed best!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Best Parameters:\n",
      "--------------------\n",
      "RandomForestRegressionModel (uid=rfr_5cf5b7f034cd) with 60 trees\n",
      "res100: org.apache.spark.ml.param.ParamMap =\n",
      "{\n",
      "\trfr_5cf5b7f034cd-cacheNodeIds: false,\n",
      "\trfr_5cf5b7f034cd-checkpointInterval: 10,\n",
      "\trfr_5cf5b7f034cd-featureSubsetStrategy: auto,\n",
      "\trfr_5cf5b7f034cd-featuresCol: features,\n",
      "\trfr_5cf5b7f034cd-impurity: variance,\n",
      "\trfr_5cf5b7f034cd-labelCol: count,\n",
      "\trfr_5cf5b7f034cd-maxBins: 32,\n",
      "\trfr_5cf5b7f034cd-maxDepth: 10,\n",
      "\trfr_5cf5b7f034cd-maxMemoryInMB: 256,\n",
      "\trfr_5cf5b7f034cd-minInfoGain: 0.0,\n",
      "\trfr_5cf5b7f034cd-minInstancesPerNode: 1,\n",
      "\trfr_5cf5b7f034cd-numTrees: 60,\n",
      "\trfr_5cf5b7f034cd-predictionCol: prediction,\n",
      "\trfr_5cf5b7f034cd-seed: 235498149,\n",
      "\trfr_5cf5b7f034cd-subsamplingRate: 1.0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "println(\"The Best Parameters:\\n--------------------\")\n",
    "println(pipelineFitted.bestModel.asInstanceOf[org.apache.spark.ml.PipelineModel].stages(0))\n",
    "pipelineFitted.bestModel.asInstanceOf[org.apache.spark.ml.PipelineModel].stages(0).extractParamMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at our holdout set results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "holdout2: org.apache.spark.sql.DataFrame = [raw_prediction: double, prediction: double ... 2 more fields]\n"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "val holdout2 = pipelineFitted.bestModel.transform(test).selectExpr(\"prediction as raw_prediction\", \"double(round(prediction)) as prediction\", \"count\", \"\"\"CASE double(round(prediction)) = count WHEN true then 1 ELSE 0 END as equal\"\"\")\n",
    "holdout2.createOrReplaceTempView(\"holdout2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_prediction</th>\n",
       "      <th>prediction</th>\n",
       "      <th>count</th>\n",
       "      <th>equal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.247139</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.834215</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.295078</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.112068</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.053147</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   raw_prediction  prediction  count  equal\n",
       "0        0.247139         0.0    0.0      1\n",
       "1        0.834215         1.0    0.0      0\n",
       "2        1.295078         1.0    1.0      1\n",
       "3        2.112068         2.0    1.0      0\n",
       "4        1.053147         1.0    1.0      1"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%spark -c sql\n",
    "select * from holdout2 limit 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as our regression metrics on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm2: org.apache.spark.mllib.evaluation.RegressionMetrics = org.apache.spark.mllib.evaluation.RegressionMetrics@1cd9777e\n",
      "MSE: 532.9772991850989\n",
      "MAE: 1.4674039580908032\n",
      "RMSE Squared: 23.086301115273944\n",
      "R Squared: 3.3375200924956605E-4\n",
      "Explained Variance: 1.2568601847723417\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "// have to do a type conversion for RegressionMetrics\n",
    "val rm2 = new RegressionMetrics(holdout2.select(\"prediction\", \"count\").rdd.map(x => (x(0).asInstanceOf[Double], x(1).asInstanceOf[Double])))\n",
    "\n",
    "println(\"MSE: \" + rm2.meanSquaredError)\n",
    "println(\"MAE: \" + rm2.meanAbsoluteError)\n",
    "println(\"RMSE Squared: \" + rm2.rootMeanSquaredError)\n",
    "println(\"R Squared: \" + rm2.r2)\n",
    "println(\"Explained Variance: \" + rm2.explainedVariance + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proportion2: org.apache.spark.sql.DataFrame = [(sum(equal) / sum(1)): double]\n"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "val proportion2 = holdout2.selectExpr(\"sum(equal)/sum(1)\")\n",
    "proportion2.createOrReplaceTempView(\"proportion2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we'll see an improvement in our \"exactly right\" proportion as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(sum(equal) / sum(1))</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.385332</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   (sum(equal) / sum(1))\n",
       "0               0.385332"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%spark -c sql\n",
    "select * from proportion2 limit 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
